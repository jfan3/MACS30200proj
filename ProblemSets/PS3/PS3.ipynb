{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not find 'nvcuda.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\\Windows\\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m           \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnvcuda_dll_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-41f098ce755e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown backend: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Perform pre-load sanity checks in order to produce a more actionable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# than we get from an error during SWIG import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mself_check\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m               \u001b[1;34m\"'C:\\\\Windows\\\\System32'. If it is not present, ensure that you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m               \u001b[1;34m\"have a CUDA-capable GPU with the correct driver installed.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m               % build_info.nvcuda_dll_name)\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m       if hasattr(build_info, \"cudart_dll_name\") and hasattr(\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find 'nvcuda.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\\Windows\\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed."
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from keras.regularizers import l2, l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### i).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "N = 100\n",
    "batchsize = 512\n",
    "epoch_count = 200\n",
    "split1 = 1/6\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "train_images = train_images.reshape( (60000, 28 * 28) )\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape( (10000, 28 * 28) )\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "fit = network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = fit.history['loss']\n",
    "training_acc = fit.history['acc']\n",
    "vali_error = fit.history['val_loss']\n",
    "vali_acc = fit.history['val_acc']\n",
    "x_vals = np.arange(1, epoch_count+1)\n",
    "\n",
    "plt.plot(x_vals, training_acc, 'r',label='Training Accuracy')\n",
    "plt.plot(x_vals, vali_acc, 'b',label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy by Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(x_vals, training_error, 'r',label='Training Accuracy')\n",
    "plt.plot(x_vals, vali_error, 'r',label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error by Epoch\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal epoch is 3\n"
     ]
    }
   ],
   "source": [
    "index_min = np.argmin(vali_error)\n",
    "print(f\"The optimal epoch is {index_min}\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_network = models.Sequential()\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "drop_network.add(layers.Dense(10, activation='softmax'))\n",
    "drop_network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "drop_fit = drop_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_val_error = drop_fit.history['val_loss']\n",
    "drop_val_acc = drop_fit.history['val_acc']\n",
    "x_vals = np.arange(1, epoch_count+1)\n",
    "\n",
    "plt.plot(x_vals, drop_val_error,'r',label='Dropped')\n",
    "plt.plot(x_vals, vali_error,'b',label='Original')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.title(\"Validation Error by epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(x_vals, drop_val_acc,'r',label='Dropped')\n",
    "plt.plot(x_vals, vali_acc,'b',label='Original')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy by epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model is doing better. \n",
    "\n",
    "### iii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build artchicture - L1 regularization# build  \n",
    "# build artchicture\n",
    "\n",
    "l1_network = models.Sequential()\n",
    "l1_network.add(layers.Dense(512, activation='relu', kernel_regularizer=l1(0.001), input_shape=(28 * 28,)))\n",
    "\n",
    "\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "\n",
    "l1_network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# build artchicture - L2 regularization\n",
    "l2_network = models.Sequential()\n",
    "l2_network.add(layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001), input_shape=(28 * 28,)))\n",
    "\n",
    "\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "\n",
    "\n",
    "l2_network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# Fit the model# Fit th \n",
    "l1_network.compile(optimizer='rmsprop', loss='categorical_crossentropy',  metrics=['acc'])\n",
    "l1_fit = l1_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n",
    "l2_network.compile(optimizer='rmsprop', loss='categorical_crossentropy',  metrics=['acc'])\n",
    "l2_fit = l2_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the training losses of all 4 models# Plot t \n",
    "l1_val_error = l1_fit.history['val_loss']\n",
    "l1_val_acc = l1_fit.history['val_acc']\n",
    "\n",
    "l2_val_error = l2_fit.history['val_loss']\n",
    "l2_val_acc = l2_fit.history['val_acc']\n",
    "\n",
    "plt.plot(x_vals, drop_val_error, label='Dropped' )\n",
    "plt.plot(x_vals, vali_error, label='Original')\n",
    "plt.plot(x_vals, l1_val_error, label='L1')\n",
    "plt.plot(x_vals, l2_val_error, label='L2')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.title(\"Validation error by Epoch across Models\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min1=np.min(drop_val_error)\n",
    "min2=np.min(vali_error)\n",
    "min3=np.min(l1_val_error)\n",
    "min4=np.min(l2_val_error)\n",
    "min1,min2,min3,min4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_epoch_count =  np.argmin(vali_error) + 1\n",
    "print(\"The optimal epoch for the main baseline model is:\", new_epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "full_fit = network.fit(train_images, train_labels, epochs=new_epoch_count, batch_size=batchsize)\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std \n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer_list):\n",
    "    '''\n",
    "    The layer_list is a list of integers, each denoting a layer to\n",
    "    add with the corresponding number of nodes\n",
    "    '''\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    for node_count in layer_list:\n",
    "        model.add(layers.Dense(node_count, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cross_validate(model_structure):\n",
    "    '''\n",
    "    Given a model structure, returns the avg MSE over 10 k-fold\n",
    "    cross validation training\n",
    "    \n",
    "    This facilitates rapid testing of models\n",
    "    '''\n",
    "    \n",
    "    kf = KFold(n_splits = k)\n",
    "    cur_fold = 0\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        cur_fold +=1 \n",
    "\n",
    "        # Partition the data\n",
    "        x_train, x_test = train_data[train_index], train_data[test_index]\n",
    "        y_train, y_test = train_targets[train_index], train_targets[test_index]\n",
    "\n",
    "\n",
    "        model = build_model(model_structure)\n",
    "        model.fit(x_train, y_train, epochs=num_epochs, batch_size=1)\n",
    "        val_mse, val_mae = model.evaluate(x_test, y_test)\n",
    "        all_scores.append(val_mse)\n",
    "        print(\"K-FOLD = {}| MSE = {}\".format(cur_fold, val_mse ))\n",
    "\n",
    "    mean_score = np.mean(all_scores)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k = 10\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 5\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use functionality to determine optimal node count, given one layer\n",
    "node_sizes = np.arange(40, 120, 10)\n",
    "scores = []\n",
    "for count in node_sizes:\n",
    "    structure = [count]\n",
    "    score = cross_validate(structure)\n",
    "    print(\"Layer 1 has {} nodes - yields MSE = {}\".format(count, score))\n",
    "    scores.append(score)\n",
    "\n",
    "min_mse_size = node_sizes[ np.argmin(scores) ]\n",
    "print(\"With 1 layer optimal node count is:\", min_mse_size)\n",
    "\n",
    "# quick plot of MSE as function of node size\n",
    "plt.plot(node_sizes, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models=[[10,20],[100,200],[10,20,30],[100,200,300],[10,20,30,40],[100,200,300,400]\n",
    "mse_scores=[]\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    score = cross_validate(model)\n",
    "    mse_scores.append(score)\n",
    "    print (f\"Model {i} with parameters {models[i]} has MSE of {score}.\")\n",
    "\n",
    "index_mse = np.argmin(mse_scores)\n",
    "print (models[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
