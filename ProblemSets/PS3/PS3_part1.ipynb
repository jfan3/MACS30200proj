{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from keras.regularizers import l2, l1\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### i).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 18s 352us/step - loss: 0.5058 - acc: 0.8359 - val_loss: 0.2153 - val_acc: 0.9337\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1588 - acc: 0.9520 - val_loss: 0.1434 - val_acc: 0.9548\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0942 - acc: 0.9709 - val_loss: 0.1446 - val_acc: 0.9576\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0644 - acc: 0.9804 - val_loss: 0.1026 - val_acc: 0.9726\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0468 - acc: 0.9860 - val_loss: 0.0803 - val_acc: 0.9797\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0381 - acc: 0.9890 - val_loss: 0.1268 - val_acc: 0.9672\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0261 - acc: 0.9917 - val_loss: 0.1660 - val_acc: 0.9662\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0247 - acc: 0.9927 - val_loss: 0.1067 - val_acc: 0.9756\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0187 - acc: 0.9942 - val_loss: 0.1012 - val_acc: 0.9803\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0166 - acc: 0.9952 - val_loss: 0.1181 - val_acc: 0.9775\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0154 - acc: 0.9955 - val_loss: 0.1122 - val_acc: 0.9770\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.1151 - val_acc: 0.9758\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.1131 - val_acc: 0.9779\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0116 - acc: 0.9968 - val_loss: 0.1637 - val_acc: 0.9739\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0109 - acc: 0.9969 - val_loss: 0.1229 - val_acc: 0.9805\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0117 - acc: 0.9972 - val_loss: 0.1232 - val_acc: 0.9805\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0104 - acc: 0.9971 - val_loss: 0.1251 - val_acc: 0.9817\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0113 - acc: 0.9971 - val_loss: 0.1218 - val_acc: 0.9800\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.1476 - val_acc: 0.9788\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0099 - acc: 0.9975 - val_loss: 0.1400 - val_acc: 0.9784\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0076 - acc: 0.9979 - val_loss: 0.1258 - val_acc: 0.9809\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.1282 - val_acc: 0.9812\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0094 - acc: 0.9978 - val_loss: 0.1296 - val_acc: 0.9817\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0090 - acc: 0.9981 - val_loss: 0.1470 - val_acc: 0.9801\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0089 - acc: 0.9978 - val_loss: 0.1260 - val_acc: 0.9818\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.1315 - val_acc: 0.9806\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0069 - acc: 0.9986 - val_loss: 0.1407 - val_acc: 0.9827\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0082 - acc: 0.9980 - val_loss: 0.1568 - val_acc: 0.9784\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0072 - acc: 0.9984 - val_loss: 0.1532 - val_acc: 0.9800\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0067 - acc: 0.9985 - val_loss: 0.1759 - val_acc: 0.9779\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0070 - acc: 0.9985 - val_loss: 0.1828 - val_acc: 0.9789\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0048 - acc: 0.9989 - val_loss: 0.1597 - val_acc: 0.9797\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0085 - acc: 0.9984 - val_loss: 0.1437 - val_acc: 0.9825\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.1562 - val_acc: 0.9797\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0084 - acc: 0.9985 - val_loss: 0.1470 - val_acc: 0.9810\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0082 - acc: 0.9985 - val_loss: 0.1744 - val_acc: 0.9806\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.1549 - val_acc: 0.9819\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0068 - acc: 0.9988 - val_loss: 0.1413 - val_acc: 0.9821\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0058 - acc: 0.9989 - val_loss: 0.1584 - val_acc: 0.9817\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.1754 - val_acc: 0.9807\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.1840 - val_acc: 0.9812\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0069 - acc: 0.9986 - val_loss: 0.1718 - val_acc: 0.9807\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0071 - acc: 0.9986 - val_loss: 0.1650 - val_acc: 0.9812\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0061 - acc: 0.9990 - val_loss: 0.1541 - val_acc: 0.9838\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0076 - acc: 0.9984 - val_loss: 0.1707 - val_acc: 0.9798\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.1656 - val_acc: 0.9829\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0047 - acc: 0.9992 - val_loss: 0.1586 - val_acc: 0.9828\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0053 - acc: 0.9990 - val_loss: 0.1848 - val_acc: 0.9801\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.1716 - val_acc: 0.9818\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0083 - acc: 0.9986 - val_loss: 0.1740 - val_acc: 0.9807\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.1854 - val_acc: 0.9804\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.1680 - val_acc: 0.9821\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.1838 - val_acc: 0.9815\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0057 - acc: 0.9990 - val_loss: 0.1761 - val_acc: 0.9819\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0061 - acc: 0.9989 - val_loss: 0.1571 - val_acc: 0.9817\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0061 - acc: 0.9990 - val_loss: 0.2280 - val_acc: 0.9769\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.1753 - val_acc: 0.9824\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0052 - acc: 0.9990 - val_loss: 0.1857 - val_acc: 0.9814\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0048 - acc: 0.9991 - val_loss: 0.1921 - val_acc: 0.9823\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.1932 - val_acc: 0.9807\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.1945 - val_acc: 0.9810\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0068 - acc: 0.9992 - val_loss: 0.1901 - val_acc: 0.9807\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.1852 - val_acc: 0.9814\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.2342 - val_acc: 0.9801\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0080 - acc: 0.9987 - val_loss: 0.1694 - val_acc: 0.9814\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0072 - acc: 0.9990 - val_loss: 0.1874 - val_acc: 0.9811\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0054 - acc: 0.9991 - val_loss: 0.1759 - val_acc: 0.9815\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0045 - acc: 0.9994 - val_loss: 0.2003 - val_acc: 0.9812\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0067 - acc: 0.9992 - val_loss: 0.2399 - val_acc: 0.9788\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0095 - acc: 0.9988 - val_loss: 0.1968 - val_acc: 0.9808\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 0.2097 - val_acc: 0.9792\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.1878 - val_acc: 0.9820\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 0.2005 - val_acc: 0.9804\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0065 - acc: 0.9990 - val_loss: 0.1941 - val_acc: 0.9823\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0071 - acc: 0.9990 - val_loss: 0.1948 - val_acc: 0.9803\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0053 - acc: 0.9992 - val_loss: 0.2743 - val_acc: 0.9752\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0079 - acc: 0.9989 - val_loss: 0.1927 - val_acc: 0.9798\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0050 - acc: 0.9993 - val_loss: 0.2076 - val_acc: 0.9819\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0065 - acc: 0.9992 - val_loss: 0.2011 - val_acc: 0.9806\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0062 - acc: 0.9990 - val_loss: 0.1832 - val_acc: 0.9815\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0070 - acc: 0.9989 - val_loss: 0.2509 - val_acc: 0.9773\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0103 - acc: 0.9987 - val_loss: 0.2602 - val_acc: 0.9748\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.2022 - val_acc: 0.9815\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0068 - acc: 0.9990 - val_loss: 0.2238 - val_acc: 0.9797\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0079 - acc: 0.9989 - val_loss: 0.2180 - val_acc: 0.9804\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0036 - acc: 0.9993 - val_loss: 0.2036 - val_acc: 0.9807\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.2834 - val_acc: 0.9769\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0084 - acc: 0.9989 - val_loss: 0.2255 - val_acc: 0.9802\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0094 - acc: 0.9986 - val_loss: 0.2127 - val_acc: 0.9800\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.2261 - val_acc: 0.9815\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0106 - acc: 0.9986 - val_loss: 0.2044 - val_acc: 0.9813\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.2201 - val_acc: 0.9818\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0058 - acc: 0.9993 - val_loss: 0.2128 - val_acc: 0.9809\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.2017 - val_acc: 0.9806\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.2257 - val_acc: 0.9796\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0047 - acc: 0.9993 - val_loss: 0.2332 - val_acc: 0.9801\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0054 - acc: 0.9993 - val_loss: 0.2116 - val_acc: 0.9817\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0078 - acc: 0.9992 - val_loss: 0.2594 - val_acc: 0.9781\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0055 - acc: 0.9991 - val_loss: 0.1854 - val_acc: 0.9830\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.2262 - val_acc: 0.9795\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0087 - acc: 0.9990 - val_loss: 0.2443 - val_acc: 0.9782\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0058 - acc: 0.9993 - val_loss: 0.2175 - val_acc: 0.9820\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0062 - acc: 0.9990 - val_loss: 0.2527 - val_acc: 0.9797\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0079 - acc: 0.9991 - val_loss: 0.1875 - val_acc: 0.9815\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.1917 - val_acc: 0.9827\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0032 - acc: 0.9996 - val_loss: 0.2093 - val_acc: 0.9823\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0065 - acc: 0.9992 - val_loss: 0.2179 - val_acc: 0.9819\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0080 - acc: 0.9991 - val_loss: 0.2228 - val_acc: 0.9814\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0057 - acc: 0.9994 - val_loss: 0.1984 - val_acc: 0.9813\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.2448 - val_acc: 0.9789\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 0.2344 - val_acc: 0.9793\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0080 - acc: 0.9991 - val_loss: 0.1943 - val_acc: 0.9819\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0063 - acc: 0.9991 - val_loss: 0.2175 - val_acc: 0.9808\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0059 - acc: 0.9991 - val_loss: 0.2202 - val_acc: 0.9794\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0071 - acc: 0.9990 - val_loss: 0.2025 - val_acc: 0.9796\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.1898 - val_acc: 0.9828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0051 - acc: 0.9994 - val_loss: 0.2071 - val_acc: 0.9818\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.2182 - val_acc: 0.9819\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.2132 - val_acc: 0.9814\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0075 - acc: 0.9992 - val_loss: 0.2153 - val_acc: 0.9826\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.2039 - val_acc: 0.9824\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0044 - acc: 0.9993 - val_loss: 0.2055 - val_acc: 0.9809\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.2249 - val_acc: 0.9815\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0074 - acc: 0.9991 - val_loss: 0.2047 - val_acc: 0.9820\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.1944 - val_acc: 0.9834\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0038 - acc: 0.9996 - val_loss: 0.2184 - val_acc: 0.9826\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0095 - acc: 0.9989 - val_loss: 0.2186 - val_acc: 0.9811\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0075 - acc: 0.9991 - val_loss: 0.2161 - val_acc: 0.9800\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.1954 - val_acc: 0.9826\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0073 - acc: 0.9991 - val_loss: 0.2035 - val_acc: 0.9824\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0060 - acc: 0.9993 - val_loss: 0.2147 - val_acc: 0.9822\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0067 - acc: 0.9993 - val_loss: 0.2301 - val_acc: 0.9812\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0063 - acc: 0.9994 - val_loss: 0.1951 - val_acc: 0.9840\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0108 - acc: 0.9989 - val_loss: 0.2056 - val_acc: 0.9826\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0084 - acc: 0.9989 - val_loss: 0.2001 - val_acc: 0.9830\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.1911 - val_acc: 0.9838\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0071 - acc: 0.9993 - val_loss: 0.2084 - val_acc: 0.9821\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0066 - acc: 0.9991 - val_loss: 0.1874 - val_acc: 0.9817\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.2470 - val_acc: 0.9794\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0089 - acc: 0.9990 - val_loss: 0.2027 - val_acc: 0.9827\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0100 - acc: 0.9990 - val_loss: 0.1905 - val_acc: 0.9829\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.2149 - val_acc: 0.9818\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0064 - acc: 0.9992 - val_loss: 0.2067 - val_acc: 0.9821\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0047 - acc: 0.9994 - val_loss: 0.2072 - val_acc: 0.9816\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0082 - acc: 0.9990 - val_loss: 0.2260 - val_acc: 0.9810\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0054 - acc: 0.9993 - val_loss: 0.2276 - val_acc: 0.9803\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0078 - acc: 0.9992 - val_loss: 0.1977 - val_acc: 0.9818\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0065 - acc: 0.9990 - val_loss: 0.2008 - val_acc: 0.9827\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0079 - acc: 0.9991 - val_loss: 0.1929 - val_acc: 0.9823\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0072 - acc: 0.9991 - val_loss: 0.2183 - val_acc: 0.9809\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.1975 - val_acc: 0.9835\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.2136 - val_acc: 0.9828\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 0.2180 - val_acc: 0.9820\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0093 - acc: 0.9991 - val_loss: 0.2176 - val_acc: 0.9810\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0083 - acc: 0.9992 - val_loss: 0.2695 - val_acc: 0.9771\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.1960 - val_acc: 0.9832\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0061 - acc: 0.9992 - val_loss: 0.2121 - val_acc: 0.9823\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0064 - acc: 0.9991 - val_loss: 0.2384 - val_acc: 0.9798\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0069 - acc: 0.9993 - val_loss: 0.2508 - val_acc: 0.9788\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0067 - acc: 0.9992 - val_loss: 0.1963 - val_acc: 0.9826\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.2292 - val_acc: 0.9811\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0034 - acc: 0.9995 - val_loss: 0.2027 - val_acc: 0.9833\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.2320 - val_acc: 0.9809\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0062 - acc: 0.9992 - val_loss: 0.1907 - val_acc: 0.9841\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0052 - acc: 0.9994 - val_loss: 0.2000 - val_acc: 0.9822\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0050 - acc: 0.9995 - val_loss: 0.2410 - val_acc: 0.9797\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0079 - acc: 0.9990 - val_loss: 0.2014 - val_acc: 0.9823\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0076 - acc: 0.9994 - val_loss: 0.2277 - val_acc: 0.9804\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 0.2381 - val_acc: 0.9802\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 0.2645 - val_acc: 0.9790\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0093 - acc: 0.9990 - val_loss: 0.1955 - val_acc: 0.9811\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0065 - acc: 0.9993 - val_loss: 0.2198 - val_acc: 0.9807\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0063 - acc: 0.9992 - val_loss: 0.2119 - val_acc: 0.9826\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0024 - acc: 0.9997 - val_loss: 0.2142 - val_acc: 0.9822\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0070 - acc: 0.9994 - val_loss: 0.2332 - val_acc: 0.9810\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0079 - acc: 0.9992 - val_loss: 0.2032 - val_acc: 0.9825\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0061 - acc: 0.9994 - val_loss: 0.2505 - val_acc: 0.9807\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0142 - acc: 0.9988 - val_loss: 0.2446 - val_acc: 0.9803\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0082 - acc: 0.9991 - val_loss: 0.2390 - val_acc: 0.9811\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 0.2566 - val_acc: 0.9796\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0036 - acc: 0.9996 - val_loss: 0.2300 - val_acc: 0.9818\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.2459 - val_acc: 0.9795\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0055 - acc: 0.9995 - val_loss: 0.2222 - val_acc: 0.9825\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0131 - acc: 0.9990 - val_loss: 0.2435 - val_acc: 0.9805\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0084 - acc: 0.9991 - val_loss: 0.2014 - val_acc: 0.9825\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0068 - acc: 0.9994 - val_loss: 0.1971 - val_acc: 0.9834\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0055 - acc: 0.9995 - val_loss: 0.2195 - val_acc: 0.9830\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0066 - acc: 0.9994 - val_loss: 0.2174 - val_acc: 0.9830\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0086 - acc: 0.9993 - val_loss: 0.2568 - val_acc: 0.9797\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0131 - acc: 0.9988 - val_loss: 0.2422 - val_acc: 0.9812\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0069 - acc: 0.9993 - val_loss: 0.2261 - val_acc: 0.9818\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0095 - acc: 0.9990 - val_loss: 0.2169 - val_acc: 0.9825\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0055 - acc: 0.9993 - val_loss: 0.2045 - val_acc: 0.9823\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0074 - acc: 0.9993 - val_loss: 0.2148 - val_acc: 0.9827\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0049 - acc: 0.9995 - val_loss: 0.2314 - val_acc: 0.9819\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0096 - acc: 0.9991 - val_loss: 0.2173 - val_acc: 0.9823\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2219 - val_acc: 0.9825\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0045 - acc: 0.9994 - val_loss: 0.2444 - val_acc: 0.9804\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0113 - acc: 0.9990 - val_loss: 0.2748 - val_acc: 0.9786\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0129 - acc: 0.9989 - val_loss: 0.2508 - val_acc: 0.9813\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "N = 100\n",
    "batchsize = 512\n",
    "epoch_count = 200\n",
    "split1 = 1/6\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "train_images = train_images.reshape( (60000, 28 * 28) )\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape( (10000, 28 * 28) )\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "fit = network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5628e6fb70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5629374c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_error = fit.history['loss']\n",
    "training_acc = fit.history['acc']\n",
    "vali_error = fit.history['val_loss']\n",
    "vali_acc = fit.history['val_acc']\n",
    "x_vals = np.arange(1, epoch_count+1)\n",
    "\n",
    "plt.plot(x_vals, training_acc, 'r',label='Training Accuracy')\n",
    "plt.plot(x_vals, vali_acc, 'b',label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy by Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(x_vals, training_error, 'r',label='Training Accuracy')\n",
    "plt.plot(x_vals, vali_error, 'r',label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error by Epoch\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal epoch is 4\n"
     ]
    }
   ],
   "source": [
    "index_min = np.argmin(vali_error)\n",
    "print(f\"The optimal epoch is {index_min}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.7421 - acc: 0.7533 - val_loss: 0.1887 - val_acc: 0.9438\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2750 - acc: 0.9185 - val_loss: 0.1443 - val_acc: 0.9592\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2039 - acc: 0.9416 - val_loss: 0.1225 - val_acc: 0.9659\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.1668 - acc: 0.9526 - val_loss: 0.1187 - val_acc: 0.9687\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1443 - acc: 0.9593 - val_loss: 0.1143 - val_acc: 0.9701\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1296 - acc: 0.9634 - val_loss: 0.0938 - val_acc: 0.9744\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1115 - acc: 0.9685 - val_loss: 0.0964 - val_acc: 0.9756\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1083 - acc: 0.9691 - val_loss: 0.0906 - val_acc: 0.9779\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.1023 - acc: 0.9707 - val_loss: 0.0862 - val_acc: 0.9780\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0919 - acc: 0.9746 - val_loss: 0.0893 - val_acc: 0.9763\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0898 - acc: 0.9752 - val_loss: 0.0880 - val_acc: 0.9775\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0903 - acc: 0.9751 - val_loss: 0.0837 - val_acc: 0.9796\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0793 - acc: 0.9784 - val_loss: 0.0836 - val_acc: 0.9792\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0792 - acc: 0.9776 - val_loss: 0.0758 - val_acc: 0.9810\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0706 - acc: 0.9798 - val_loss: 0.0816 - val_acc: 0.9816\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0737 - acc: 0.9803 - val_loss: 0.0861 - val_acc: 0.9804\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0693 - acc: 0.9811 - val_loss: 0.0837 - val_acc: 0.9802\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0692 - acc: 0.9810 - val_loss: 0.0780 - val_acc: 0.9819\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0654 - acc: 0.9819 - val_loss: 0.0843 - val_acc: 0.9811\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0657 - acc: 0.9831 - val_loss: 0.0837 - val_acc: 0.9819\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0652 - acc: 0.9824 - val_loss: 0.0812 - val_acc: 0.9822\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0637 - acc: 0.9832 - val_loss: 0.0965 - val_acc: 0.9802\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0586 - acc: 0.9849 - val_loss: 0.0909 - val_acc: 0.9818\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0611 - acc: 0.9848 - val_loss: 0.0915 - val_acc: 0.9818\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0627 - acc: 0.9841 - val_loss: 0.0869 - val_acc: 0.9825\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0609 - acc: 0.9853 - val_loss: 0.0872 - val_acc: 0.9817\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0591 - acc: 0.9854 - val_loss: 0.0906 - val_acc: 0.9832\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0554 - acc: 0.9857 - val_loss: 0.0996 - val_acc: 0.9814\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0565 - acc: 0.9858 - val_loss: 0.0902 - val_acc: 0.9822\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0578 - acc: 0.9859 - val_loss: 0.0904 - val_acc: 0.9815\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0544 - acc: 0.9868 - val_loss: 0.0867 - val_acc: 0.9819\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0547 - acc: 0.9863 - val_loss: 0.0885 - val_acc: 0.9823\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0542 - acc: 0.9867 - val_loss: 0.0860 - val_acc: 0.9841\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0523 - acc: 0.9872 - val_loss: 0.0997 - val_acc: 0.9825\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0546 - acc: 0.9866 - val_loss: 0.0955 - val_acc: 0.9822\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0553 - acc: 0.9869 - val_loss: 0.0940 - val_acc: 0.9832\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0575 - acc: 0.9867 - val_loss: 0.0883 - val_acc: 0.9835\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0526 - acc: 0.9877 - val_loss: 0.0941 - val_acc: 0.9812\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0529 - acc: 0.9874 - val_loss: 0.1027 - val_acc: 0.9822\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0510 - acc: 0.9881 - val_loss: 0.0990 - val_acc: 0.9826\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0546 - acc: 0.9876 - val_loss: 0.0966 - val_acc: 0.9818\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0549 - acc: 0.9882 - val_loss: 0.0885 - val_acc: 0.9832\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0490 - acc: 0.9885 - val_loss: 0.0982 - val_acc: 0.9832\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0533 - acc: 0.9878 - val_loss: 0.0985 - val_acc: 0.9833\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0545 - acc: 0.9875 - val_loss: 0.0977 - val_acc: 0.9832\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0532 - acc: 0.9884 - val_loss: 0.0933 - val_acc: 0.9822\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0496 - acc: 0.9890 - val_loss: 0.0867 - val_acc: 0.9844\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0484 - acc: 0.9892 - val_loss: 0.0954 - val_acc: 0.9829\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0484 - acc: 0.9891 - val_loss: 0.1037 - val_acc: 0.9821\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0499 - acc: 0.9884 - val_loss: 0.0952 - val_acc: 0.9818\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0503 - acc: 0.9889 - val_loss: 0.1065 - val_acc: 0.9818\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0448 - acc: 0.9899 - val_loss: 0.0991 - val_acc: 0.9838\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0481 - acc: 0.9892 - val_loss: 0.0976 - val_acc: 0.9828\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0512 - acc: 0.9897 - val_loss: 0.0952 - val_acc: 0.9835\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0513 - acc: 0.9893 - val_loss: 0.0961 - val_acc: 0.9825\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0502 - acc: 0.9891 - val_loss: 0.0999 - val_acc: 0.9812\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.0500 - acc: 0.9895 - val_loss: 0.1015 - val_acc: 0.9825\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 2s 32us/step - loss: 0.0509 - acc: 0.9899 - val_loss: 0.1041 - val_acc: 0.9835\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.0477 - acc: 0.9899 - val_loss: 0.1040 - val_acc: 0.9830\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0508 - acc: 0.9895 - val_loss: 0.0984 - val_acc: 0.9841\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0461 - acc: 0.9903 - val_loss: 0.1122 - val_acc: 0.9814\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0489 - acc: 0.9899 - val_loss: 0.0979 - val_acc: 0.9836\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0469 - acc: 0.9904 - val_loss: 0.1097 - val_acc: 0.9820\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0523 - acc: 0.9897 - val_loss: 0.1127 - val_acc: 0.9824\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0503 - acc: 0.9898 - val_loss: 0.1017 - val_acc: 0.9831\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0483 - acc: 0.9897 - val_loss: 0.0936 - val_acc: 0.9840\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0486 - acc: 0.9897 - val_loss: 0.1068 - val_acc: 0.9831\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0455 - acc: 0.9912 - val_loss: 0.1106 - val_acc: 0.9823\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0489 - acc: 0.9898 - val_loss: 0.1185 - val_acc: 0.9820\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0511 - acc: 0.9904 - val_loss: 0.1066 - val_acc: 0.9825\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0543 - acc: 0.9895 - val_loss: 0.1006 - val_acc: 0.9828\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0471 - acc: 0.9903 - val_loss: 0.1045 - val_acc: 0.9840\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0501 - acc: 0.9905 - val_loss: 0.1084 - val_acc: 0.9836\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0435 - acc: 0.9912 - val_loss: 0.1210 - val_acc: 0.9816\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0521 - acc: 0.9902 - val_loss: 0.1198 - val_acc: 0.9833\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0483 - acc: 0.9907 - val_loss: 0.1099 - val_acc: 0.9832\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0494 - acc: 0.9912 - val_loss: 0.1106 - val_acc: 0.9826\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0458 - acc: 0.9910 - val_loss: 0.1076 - val_acc: 0.9838\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0483 - acc: 0.9906 - val_loss: 0.1178 - val_acc: 0.9829\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0519 - acc: 0.9905 - val_loss: 0.1078 - val_acc: 0.9829\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0542 - acc: 0.9904 - val_loss: 0.1078 - val_acc: 0.9831\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0478 - acc: 0.9914 - val_loss: 0.1188 - val_acc: 0.9824\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0553 - acc: 0.9906 - val_loss: 0.1217 - val_acc: 0.9830\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0544 - acc: 0.9903 - val_loss: 0.1178 - val_acc: 0.9838\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0472 - acc: 0.9917 - val_loss: 0.1188 - val_acc: 0.9823\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0513 - acc: 0.9912 - val_loss: 0.1190 - val_acc: 0.9833\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0535 - acc: 0.9908 - val_loss: 0.1104 - val_acc: 0.9842\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0572 - acc: 0.9904 - val_loss: 0.1119 - val_acc: 0.9846\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0538 - acc: 0.9906 - val_loss: 0.1104 - val_acc: 0.9837\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0512 - acc: 0.9910 - val_loss: 0.1153 - val_acc: 0.9828\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0522 - acc: 0.9910 - val_loss: 0.1090 - val_acc: 0.9843\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0576 - acc: 0.9903 - val_loss: 0.1078 - val_acc: 0.9830\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0562 - acc: 0.9902 - val_loss: 0.1170 - val_acc: 0.9827\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0549 - acc: 0.9905 - val_loss: 0.1097 - val_acc: 0.9837\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0558 - acc: 0.9908 - val_loss: 0.1249 - val_acc: 0.9831\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0567 - acc: 0.9914 - val_loss: 0.1193 - val_acc: 0.9832\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0569 - acc: 0.9904 - val_loss: 0.1303 - val_acc: 0.9826\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0528 - acc: 0.9913 - val_loss: 0.1289 - val_acc: 0.9839\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0512 - acc: 0.9912 - val_loss: 0.1233 - val_acc: 0.9827\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0591 - acc: 0.9905 - val_loss: 0.1169 - val_acc: 0.9839\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0557 - acc: 0.9908 - val_loss: 0.1339 - val_acc: 0.9830\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0565 - acc: 0.9911 - val_loss: 0.1214 - val_acc: 0.9842\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0525 - acc: 0.9913 - val_loss: 0.1280 - val_acc: 0.9818\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0531 - acc: 0.9913 - val_loss: 0.1272 - val_acc: 0.9837\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0537 - acc: 0.9916 - val_loss: 0.1251 - val_acc: 0.9842\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0580 - acc: 0.9907 - val_loss: 0.1388 - val_acc: 0.9819\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0543 - acc: 0.9911 - val_loss: 0.1211 - val_acc: 0.9835\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0495 - acc: 0.9918 - val_loss: 0.1368 - val_acc: 0.9835\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0545 - acc: 0.9917 - val_loss: 0.1306 - val_acc: 0.9832\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0577 - acc: 0.9908 - val_loss: 0.1234 - val_acc: 0.9832\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0584 - acc: 0.9911 - val_loss: 0.1261 - val_acc: 0.9823\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0529 - acc: 0.9914 - val_loss: 0.1324 - val_acc: 0.9829\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0566 - acc: 0.9915 - val_loss: 0.1302 - val_acc: 0.9830\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0566 - acc: 0.9911 - val_loss: 0.1452 - val_acc: 0.9822\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0600 - acc: 0.9910 - val_loss: 0.1196 - val_acc: 0.9848\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0562 - acc: 0.9918 - val_loss: 0.1366 - val_acc: 0.9827\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0515 - acc: 0.9914 - val_loss: 0.1301 - val_acc: 0.9836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0638 - acc: 0.9906 - val_loss: 0.1233 - val_acc: 0.9846\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.0591 - acc: 0.9912 - val_loss: 0.1378 - val_acc: 0.9846\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0526 - acc: 0.9914 - val_loss: 0.1303 - val_acc: 0.9831\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0604 - acc: 0.9911 - val_loss: 0.1378 - val_acc: 0.9833\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0591 - acc: 0.9911 - val_loss: 0.1389 - val_acc: 0.9819\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0585 - acc: 0.9910 - val_loss: 0.1290 - val_acc: 0.9839\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.0582 - acc: 0.9920 - val_loss: 0.1531 - val_acc: 0.9821\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0651 - acc: 0.9910 - val_loss: 0.1341 - val_acc: 0.9827\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0604 - acc: 0.9913 - val_loss: 0.1333 - val_acc: 0.9833\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0590 - acc: 0.9914 - val_loss: 0.1525 - val_acc: 0.9824\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0620 - acc: 0.9911 - val_loss: 0.1280 - val_acc: 0.9833\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0583 - acc: 0.9916 - val_loss: 0.1325 - val_acc: 0.9820\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0555 - acc: 0.9916 - val_loss: 0.1237 - val_acc: 0.9836\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0606 - acc: 0.9915 - val_loss: 0.1282 - val_acc: 0.9825\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0593 - acc: 0.9916 - val_loss: 0.1332 - val_acc: 0.9837\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.0691 - acc: 0.9910 - val_loss: 0.1396 - val_acc: 0.9813\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0568 - acc: 0.9911 - val_loss: 0.1352 - val_acc: 0.9823\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0630 - acc: 0.9915 - val_loss: 0.1418 - val_acc: 0.9832\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0670 - acc: 0.9914 - val_loss: 0.1522 - val_acc: 0.9822\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.0646 - acc: 0.9920 - val_loss: 0.1612 - val_acc: 0.9825\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0709 - acc: 0.9913 - val_loss: 0.1563 - val_acc: 0.9825\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0676 - acc: 0.9914 - val_loss: 0.1490 - val_acc: 0.9817\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0595 - acc: 0.9919 - val_loss: 0.1525 - val_acc: 0.9826\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0602 - acc: 0.9921 - val_loss: 0.1499 - val_acc: 0.9833\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0680 - acc: 0.9913 - val_loss: 0.1427 - val_acc: 0.9830\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0635 - acc: 0.9916 - val_loss: 0.1471 - val_acc: 0.9823\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0682 - acc: 0.9914 - val_loss: 0.1636 - val_acc: 0.9834\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0679 - acc: 0.9912 - val_loss: 0.1564 - val_acc: 0.9833\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0680 - acc: 0.9909 - val_loss: 0.1404 - val_acc: 0.9851\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0689 - acc: 0.9912 - val_loss: 0.1490 - val_acc: 0.9842\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0669 - acc: 0.9913 - val_loss: 0.1454 - val_acc: 0.9837\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0726 - acc: 0.9910 - val_loss: 0.1333 - val_acc: 0.9842\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0603 - acc: 0.9917 - val_loss: 0.1355 - val_acc: 0.9833\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0667 - acc: 0.9912 - val_loss: 0.1453 - val_acc: 0.9837\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0689 - acc: 0.9914 - val_loss: 0.1371 - val_acc: 0.9840\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0654 - acc: 0.9917 - val_loss: 0.1409 - val_acc: 0.9838\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0691 - acc: 0.9911 - val_loss: 0.1467 - val_acc: 0.9843\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0733 - acc: 0.9916 - val_loss: 0.1529 - val_acc: 0.9833\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.0706 - acc: 0.9915 - val_loss: 0.1476 - val_acc: 0.9832\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0612 - acc: 0.9919 - val_loss: 0.1423 - val_acc: 0.9839\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0714 - acc: 0.9921 - val_loss: 0.1469 - val_acc: 0.9838\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0755 - acc: 0.9910 - val_loss: 0.1407 - val_acc: 0.9839\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0698 - acc: 0.9916 - val_loss: 0.1526 - val_acc: 0.9830\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0664 - acc: 0.9914 - val_loss: 0.1515 - val_acc: 0.9833\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0689 - acc: 0.9916 - val_loss: 0.1472 - val_acc: 0.9831\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0831 - acc: 0.9911 - val_loss: 0.1465 - val_acc: 0.9834\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0696 - acc: 0.9920 - val_loss: 0.1542 - val_acc: 0.9832\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 2s 32us/step - loss: 0.0794 - acc: 0.9908 - val_loss: 0.1435 - val_acc: 0.9832\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0682 - acc: 0.9921 - val_loss: 0.1493 - val_acc: 0.9832\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0702 - acc: 0.9917 - val_loss: 0.1461 - val_acc: 0.9828\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0704 - acc: 0.9918 - val_loss: 0.1436 - val_acc: 0.9837\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.0748 - acc: 0.9918 - val_loss: 0.1479 - val_acc: 0.9838\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0706 - acc: 0.9912 - val_loss: 0.1312 - val_acc: 0.9845\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0675 - acc: 0.9919 - val_loss: 0.1636 - val_acc: 0.9830\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0714 - acc: 0.9916 - val_loss: 0.1555 - val_acc: 0.9841\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0779 - acc: 0.9918 - val_loss: 0.1597 - val_acc: 0.9829\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.0798 - acc: 0.9910 - val_loss: 0.1589 - val_acc: 0.9836\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0832 - acc: 0.9906 - val_loss: 0.1578 - val_acc: 0.9827\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0687 - acc: 0.9914 - val_loss: 0.1507 - val_acc: 0.9837\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0880 - acc: 0.9902 - val_loss: 0.1533 - val_acc: 0.9832\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0686 - acc: 0.9915 - val_loss: 0.1561 - val_acc: 0.9837\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0821 - acc: 0.9910 - val_loss: 0.1769 - val_acc: 0.9824\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0730 - acc: 0.9915 - val_loss: 0.1566 - val_acc: 0.9835\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0730 - acc: 0.9911 - val_loss: 0.1557 - val_acc: 0.9835\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0780 - acc: 0.9909 - val_loss: 0.1550 - val_acc: 0.9839\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.0694 - acc: 0.9920 - val_loss: 0.1634 - val_acc: 0.9832\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0742 - acc: 0.9914 - val_loss: 0.1589 - val_acc: 0.9823\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0754 - acc: 0.9920 - val_loss: 0.1613 - val_acc: 0.9837\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0806 - acc: 0.9909 - val_loss: 0.1664 - val_acc: 0.9832\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0806 - acc: 0.9913 - val_loss: 0.1751 - val_acc: 0.9831\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0862 - acc: 0.9912 - val_loss: 0.1642 - val_acc: 0.9835\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0782 - acc: 0.9913 - val_loss: 0.1721 - val_acc: 0.9828\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0819 - acc: 0.9918 - val_loss: 0.1615 - val_acc: 0.9833\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0775 - acc: 0.9916 - val_loss: 0.1663 - val_acc: 0.9825\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0772 - acc: 0.9915 - val_loss: 0.1809 - val_acc: 0.9819\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.0799 - acc: 0.9915 - val_loss: 0.1694 - val_acc: 0.9820\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0732 - acc: 0.9916 - val_loss: 0.1548 - val_acc: 0.9850\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0762 - acc: 0.9915 - val_loss: 0.1591 - val_acc: 0.9835\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0738 - acc: 0.9919 - val_loss: 0.1629 - val_acc: 0.9846\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0777 - acc: 0.9914 - val_loss: 0.1717 - val_acc: 0.9831\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0769 - acc: 0.9916 - val_loss: 0.1651 - val_acc: 0.9833\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0785 - acc: 0.9914 - val_loss: 0.1736 - val_acc: 0.9824\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0796 - acc: 0.9919 - val_loss: 0.1671 - val_acc: 0.9842\n"
     ]
    }
   ],
   "source": [
    "drop_network = models.Sequential()\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "drop_network.add(layers.Dense(10, activation='softmax'))\n",
    "drop_network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "drop_fit = drop_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f561437c198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f563c97bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_val_error = drop_fit.history['val_loss']\n",
    "drop_val_acc = drop_fit.history['val_acc']\n",
    "x_vals = np.arange(1, epoch_count+1)\n",
    "\n",
    "plt.plot(x_vals, drop_val_error,'r',label='Dropped')\n",
    "plt.plot(x_vals, vali_error,'b',label='Original')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.title(\"Validation Error by epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(x_vals, drop_val_acc,'r',label='Dropped')\n",
    "plt.plot(x_vals, vali_acc,'b',label='Original')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy by epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model is doing better. \n",
    "\n",
    "### iii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 13.9564 - acc: 0.6596 - val_loss: 4.0697 - val_acc: 0.7748\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 3.1013 - acc: 0.7827 - val_loss: 2.5772 - val_acc: 0.7748\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 2.2231 - acc: 0.8284 - val_loss: 2.0785 - val_acc: 0.7984\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.8953 - acc: 0.8488 - val_loss: 1.7239 - val_acc: 0.8792\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.7002 - acc: 0.8696 - val_loss: 1.6180 - val_acc: 0.8770\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.5698 - acc: 0.8832 - val_loss: 1.4944 - val_acc: 0.8958\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.4818 - acc: 0.8898 - val_loss: 1.3936 - val_acc: 0.9123\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.4115 - acc: 0.8970 - val_loss: 1.3659 - val_acc: 0.9091\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 1.3570 - acc: 0.9055 - val_loss: 1.3992 - val_acc: 0.8837\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.3107 - acc: 0.9112 - val_loss: 1.2505 - val_acc: 0.9269\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.2794 - acc: 0.9139 - val_loss: 1.2238 - val_acc: 0.9306\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.2476 - acc: 0.9197 - val_loss: 1.2324 - val_acc: 0.9233\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.2295 - acc: 0.9212 - val_loss: 1.2592 - val_acc: 0.9085\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.2095 - acc: 0.9239 - val_loss: 1.2005 - val_acc: 0.9240\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.1890 - acc: 0.9268 - val_loss: 1.1564 - val_acc: 0.9366\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.1714 - acc: 0.9281 - val_loss: 1.1277 - val_acc: 0.9417\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.1527 - acc: 0.9317 - val_loss: 1.1578 - val_acc: 0.9284\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.1414 - acc: 0.9318 - val_loss: 1.1219 - val_acc: 0.9385\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.1310 - acc: 0.9328 - val_loss: 1.1106 - val_acc: 0.9407\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.1211 - acc: 0.9339 - val_loss: 1.0957 - val_acc: 0.9399\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 1.1131 - acc: 0.9341 - val_loss: 1.0879 - val_acc: 0.9423\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0998 - acc: 0.9367 - val_loss: 1.0835 - val_acc: 0.9433\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.0953 - acc: 0.9356 - val_loss: 1.0576 - val_acc: 0.9471\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.0851 - acc: 0.9370 - val_loss: 1.0728 - val_acc: 0.9424\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0785 - acc: 0.9379 - val_loss: 1.0776 - val_acc: 0.9374\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 1.0700 - acc: 0.9386 - val_loss: 1.0704 - val_acc: 0.9363\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 1.0663 - acc: 0.9388 - val_loss: 1.0592 - val_acc: 0.9406\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 1.0571 - acc: 0.9406 - val_loss: 1.1047 - val_acc: 0.9204\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.0529 - acc: 0.9408 - val_loss: 1.0449 - val_acc: 0.9435\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.0484 - acc: 0.9406 - val_loss: 1.0830 - val_acc: 0.9320\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 1.0414 - acc: 0.9418 - val_loss: 1.0341 - val_acc: 0.9437\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.0385 - acc: 0.9417 - val_loss: 1.0201 - val_acc: 0.9480\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.0325 - acc: 0.9411 - val_loss: 1.0385 - val_acc: 0.9410\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0295 - acc: 0.9425 - val_loss: 1.0150 - val_acc: 0.9478\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.0256 - acc: 0.9427 - val_loss: 1.0255 - val_acc: 0.9435\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.0236 - acc: 0.9432 - val_loss: 1.0122 - val_acc: 0.9434\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0209 - acc: 0.9428 - val_loss: 0.9980 - val_acc: 0.9487\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0146 - acc: 0.9447 - val_loss: 1.0470 - val_acc: 0.9330\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0132 - acc: 0.9438 - val_loss: 1.0075 - val_acc: 0.9429\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0110 - acc: 0.9445 - val_loss: 1.0009 - val_acc: 0.9478\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 1.0081 - acc: 0.9445 - val_loss: 1.0441 - val_acc: 0.9302\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0095 - acc: 0.9445 - val_loss: 0.9937 - val_acc: 0.9493\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0012 - acc: 0.9449 - val_loss: 1.0308 - val_acc: 0.9347\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 1.0028 - acc: 0.9447 - val_loss: 0.9878 - val_acc: 0.9495\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9997 - acc: 0.9460 - val_loss: 1.0034 - val_acc: 0.9433\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.9980 - acc: 0.9460 - val_loss: 1.0068 - val_acc: 0.9436\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9958 - acc: 0.9456 - val_loss: 1.0100 - val_acc: 0.9391\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9953 - acc: 0.9459 - val_loss: 0.9919 - val_acc: 0.9447\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9907 - acc: 0.9458 - val_loss: 0.9704 - val_acc: 0.9507\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9899 - acc: 0.9467 - val_loss: 0.9965 - val_acc: 0.9478\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9888 - acc: 0.9450 - val_loss: 0.9779 - val_acc: 0.9495\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9868 - acc: 0.9466 - val_loss: 0.9704 - val_acc: 0.9530\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9839 - acc: 0.9470 - val_loss: 0.9580 - val_acc: 0.9563\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9837 - acc: 0.9464 - val_loss: 0.9567 - val_acc: 0.9539\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9811 - acc: 0.9464 - val_loss: 0.9886 - val_acc: 0.9431\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9805 - acc: 0.9466 - val_loss: 1.0001 - val_acc: 0.9417\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9809 - acc: 0.9468 - val_loss: 1.0006 - val_acc: 0.9433\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9770 - acc: 0.9470 - val_loss: 1.0251 - val_acc: 0.9336\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9770 - acc: 0.9469 - val_loss: 0.9597 - val_acc: 0.9533\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9734 - acc: 0.9478 - val_loss: 0.9552 - val_acc: 0.9551\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9707 - acc: 0.9486 - val_loss: 0.9567 - val_acc: 0.9549\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9756 - acc: 0.9462 - val_loss: 0.9706 - val_acc: 0.9481\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9704 - acc: 0.9481 - val_loss: 0.9870 - val_acc: 0.9452\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.9686 - acc: 0.9487 - val_loss: 0.9700 - val_acc: 0.9484\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9678 - acc: 0.9499 - val_loss: 0.9893 - val_acc: 0.9429\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9676 - acc: 0.9479 - val_loss: 0.9434 - val_acc: 0.9579\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9638 - acc: 0.9494 - val_loss: 0.9622 - val_acc: 0.9490\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9654 - acc: 0.9490 - val_loss: 0.9579 - val_acc: 0.9530\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9655 - acc: 0.9482 - val_loss: 0.9493 - val_acc: 0.9537\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9620 - acc: 0.9500 - val_loss: 0.9823 - val_acc: 0.9443\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9629 - acc: 0.9484 - val_loss: 0.9727 - val_acc: 0.9453\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9611 - acc: 0.9501 - val_loss: 0.9418 - val_acc: 0.9555\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9589 - acc: 0.9499 - val_loss: 0.9630 - val_acc: 0.9498\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9609 - acc: 0.9488 - val_loss: 0.9482 - val_acc: 0.9532\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9600 - acc: 0.9496 - val_loss: 0.9529 - val_acc: 0.9503\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9579 - acc: 0.9496 - val_loss: 0.9423 - val_acc: 0.9529\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9565 - acc: 0.9508 - val_loss: 0.9313 - val_acc: 0.9587\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9521 - acc: 0.9511 - val_loss: 0.9729 - val_acc: 0.9450\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9523 - acc: 0.9512 - val_loss: 0.9309 - val_acc: 0.9574\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9546 - acc: 0.9499 - val_loss: 0.9717 - val_acc: 0.9453\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9516 - acc: 0.9495 - val_loss: 0.9774 - val_acc: 0.9430\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9531 - acc: 0.9490 - val_loss: 0.9304 - val_acc: 0.9582\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.9502 - acc: 0.9502 - val_loss: 0.9512 - val_acc: 0.9484\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9505 - acc: 0.9497 - val_loss: 0.9798 - val_acc: 0.9401\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9501 - acc: 0.9500 - val_loss: 0.9616 - val_acc: 0.9480\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.9461 - acc: 0.9506 - val_loss: 0.9483 - val_acc: 0.9508\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9487 - acc: 0.9497 - val_loss: 0.9457 - val_acc: 0.9501\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9442 - acc: 0.9518 - val_loss: 0.9331 - val_acc: 0.9559\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9448 - acc: 0.9512 - val_loss: 0.9764 - val_acc: 0.9412\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9446 - acc: 0.9497 - val_loss: 0.9299 - val_acc: 0.9566\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9428 - acc: 0.9509 - val_loss: 0.9497 - val_acc: 0.9515\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9420 - acc: 0.9504 - val_loss: 0.9540 - val_acc: 0.9473\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9399 - acc: 0.9511 - val_loss: 0.9456 - val_acc: 0.9518\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9423 - acc: 0.9507 - val_loss: 0.9471 - val_acc: 0.9504\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9395 - acc: 0.9514 - val_loss: 0.9244 - val_acc: 0.9569\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9383 - acc: 0.9518 - val_loss: 0.9954 - val_acc: 0.9350\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9412 - acc: 0.9509 - val_loss: 1.0538 - val_acc: 0.9208\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9358 - acc: 0.9524 - val_loss: 0.9295 - val_acc: 0.9577\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9384 - acc: 0.9515 - val_loss: 0.9295 - val_acc: 0.9543\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9362 - acc: 0.9532 - val_loss: 0.9433 - val_acc: 0.9469\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.9367 - acc: 0.9518 - val_loss: 0.9394 - val_acc: 0.9533\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9369 - acc: 0.9507 - val_loss: 0.9268 - val_acc: 0.9563\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9339 - acc: 0.9524 - val_loss: 0.9436 - val_acc: 0.9480\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9357 - acc: 0.9511 - val_loss: 0.9281 - val_acc: 0.9549\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9328 - acc: 0.9521 - val_loss: 0.9254 - val_acc: 0.9539\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9343 - acc: 0.9517 - val_loss: 0.9429 - val_acc: 0.9508\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9315 - acc: 0.9525 - val_loss: 0.9283 - val_acc: 0.9520\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9318 - acc: 0.9523 - val_loss: 0.9147 - val_acc: 0.9579\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9334 - acc: 0.9520 - val_loss: 0.9413 - val_acc: 0.9527\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9297 - acc: 0.9526 - val_loss: 0.9192 - val_acc: 0.9592\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.9294 - acc: 0.9530 - val_loss: 0.9385 - val_acc: 0.9525\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9310 - acc: 0.9526 - val_loss: 0.9431 - val_acc: 0.9509\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9310 - acc: 0.9515 - val_loss: 0.9342 - val_acc: 0.9529\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9263 - acc: 0.9543 - val_loss: 0.9345 - val_acc: 0.9513\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9258 - acc: 0.9538 - val_loss: 0.9227 - val_acc: 0.9562\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.9269 - acc: 0.9534 - val_loss: 0.9354 - val_acc: 0.9498\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9262 - acc: 0.9521 - val_loss: 0.9193 - val_acc: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9287 - acc: 0.9535 - val_loss: 0.9571 - val_acc: 0.9443\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9287 - acc: 0.9522 - val_loss: 0.9220 - val_acc: 0.9568\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9251 - acc: 0.9537 - val_loss: 1.0169 - val_acc: 0.9268\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9261 - acc: 0.9526 - val_loss: 0.9447 - val_acc: 0.9491\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9232 - acc: 0.9532 - val_loss: 0.9760 - val_acc: 0.9388\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9213 - acc: 0.9543 - val_loss: 0.9093 - val_acc: 0.9604\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9244 - acc: 0.9536 - val_loss: 0.9468 - val_acc: 0.9467\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9237 - acc: 0.9531 - val_loss: 0.9405 - val_acc: 0.9451\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9208 - acc: 0.9542 - val_loss: 0.9356 - val_acc: 0.9505\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9204 - acc: 0.9541 - val_loss: 0.9427 - val_acc: 0.9485\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9236 - acc: 0.9527 - val_loss: 0.9285 - val_acc: 0.9512\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9184 - acc: 0.9547 - val_loss: 0.9270 - val_acc: 0.9529\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.9196 - acc: 0.9533 - val_loss: 0.9885 - val_acc: 0.9369\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9184 - acc: 0.9552 - val_loss: 0.9299 - val_acc: 0.9504\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9185 - acc: 0.9538 - val_loss: 0.9280 - val_acc: 0.9525\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9164 - acc: 0.9552 - val_loss: 0.9304 - val_acc: 0.9504\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9168 - acc: 0.9542 - val_loss: 0.9398 - val_acc: 0.9485\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9188 - acc: 0.9536 - val_loss: 0.9161 - val_acc: 0.9559\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9213 - acc: 0.9520 - val_loss: 0.9173 - val_acc: 0.9550\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9127 - acc: 0.9544 - val_loss: 0.9488 - val_acc: 0.9484\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9150 - acc: 0.9554 - val_loss: 0.9091 - val_acc: 0.9586\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9141 - acc: 0.9549 - val_loss: 0.9123 - val_acc: 0.9563\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9154 - acc: 0.9547 - val_loss: 0.9084 - val_acc: 0.9587\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9132 - acc: 0.9551 - val_loss: 0.9127 - val_acc: 0.9574\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9135 - acc: 0.9556 - val_loss: 0.9328 - val_acc: 0.9481\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9118 - acc: 0.9559 - val_loss: 0.9126 - val_acc: 0.9545\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9124 - acc: 0.9559 - val_loss: 0.9127 - val_acc: 0.9562\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9120 - acc: 0.9555 - val_loss: 0.9344 - val_acc: 0.9488\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9102 - acc: 0.9563 - val_loss: 0.9008 - val_acc: 0.9592\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9096 - acc: 0.9558 - val_loss: 0.9197 - val_acc: 0.9552\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9095 - acc: 0.9566 - val_loss: 0.9070 - val_acc: 0.9585\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9118 - acc: 0.9548 - val_loss: 0.9065 - val_acc: 0.9580\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9087 - acc: 0.9565 - val_loss: 0.9001 - val_acc: 0.9587\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9068 - acc: 0.9573 - val_loss: 0.9399 - val_acc: 0.9464\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9094 - acc: 0.9554 - val_loss: 0.9227 - val_acc: 0.9527\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9084 - acc: 0.9561 - val_loss: 0.9595 - val_acc: 0.9409\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9106 - acc: 0.9557 - val_loss: 0.9038 - val_acc: 0.9575\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9082 - acc: 0.9551 - val_loss: 0.9343 - val_acc: 0.9464\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9076 - acc: 0.9565 - val_loss: 0.9762 - val_acc: 0.9354\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9085 - acc: 0.9551 - val_loss: 0.9087 - val_acc: 0.9543\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9097 - acc: 0.9554 - val_loss: 0.9275 - val_acc: 0.9521\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9047 - acc: 0.9570 - val_loss: 0.9009 - val_acc: 0.9593\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9067 - acc: 0.9559 - val_loss: 0.9343 - val_acc: 0.9496\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9073 - acc: 0.9557 - val_loss: 0.8936 - val_acc: 0.9615\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9050 - acc: 0.9569 - val_loss: 0.9171 - val_acc: 0.9551\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9055 - acc: 0.9562 - val_loss: 0.8966 - val_acc: 0.9592\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9029 - acc: 0.9562 - val_loss: 0.9122 - val_acc: 0.9553\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9052 - acc: 0.9563 - val_loss: 0.8988 - val_acc: 0.9593\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9050 - acc: 0.9561 - val_loss: 0.9194 - val_acc: 0.9498\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9062 - acc: 0.9561 - val_loss: 0.9401 - val_acc: 0.9463\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9025 - acc: 0.9573 - val_loss: 0.9247 - val_acc: 0.9517\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9005 - acc: 0.9575 - val_loss: 0.9085 - val_acc: 0.9546\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9037 - acc: 0.9565 - val_loss: 0.9014 - val_acc: 0.9590\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9043 - acc: 0.9559 - val_loss: 0.9184 - val_acc: 0.9536\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.9036 - acc: 0.9565 - val_loss: 0.9680 - val_acc: 0.9360\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9027 - acc: 0.9572 - val_loss: 0.9134 - val_acc: 0.9509\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9016 - acc: 0.9565 - val_loss: 0.9012 - val_acc: 0.9574\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.9028 - acc: 0.9574 - val_loss: 0.8984 - val_acc: 0.9594\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9018 - acc: 0.9567 - val_loss: 0.9024 - val_acc: 0.9571\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.8975 - acc: 0.9588 - val_loss: 0.9074 - val_acc: 0.9562\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9010 - acc: 0.9582 - val_loss: 0.9414 - val_acc: 0.9465\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8982 - acc: 0.9580 - val_loss: 0.9338 - val_acc: 0.9485\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8986 - acc: 0.9565 - val_loss: 0.9666 - val_acc: 0.9402\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.8997 - acc: 0.9576 - val_loss: 0.9073 - val_acc: 0.9575\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.9008 - acc: 0.9563 - val_loss: 0.9037 - val_acc: 0.9580\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8980 - acc: 0.9576 - val_loss: 0.9209 - val_acc: 0.9523\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8969 - acc: 0.9578 - val_loss: 0.8985 - val_acc: 0.9603\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8984 - acc: 0.9577 - val_loss: 0.9032 - val_acc: 0.9545\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.8995 - acc: 0.9567 - val_loss: 0.9019 - val_acc: 0.9570\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8973 - acc: 0.9579 - val_loss: 0.9067 - val_acc: 0.9560\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.8968 - acc: 0.9578 - val_loss: 0.9785 - val_acc: 0.9346\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8991 - acc: 0.9575 - val_loss: 0.9340 - val_acc: 0.9484\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.8971 - acc: 0.9574 - val_loss: 0.9271 - val_acc: 0.9460\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.8971 - acc: 0.9583 - val_loss: 0.9444 - val_acc: 0.9437\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8952 - acc: 0.9581 - val_loss: 0.9015 - val_acc: 0.9582\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.8958 - acc: 0.9577 - val_loss: 0.9242 - val_acc: 0.9533\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8991 - acc: 0.9572 - val_loss: 0.8945 - val_acc: 0.9579\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8964 - acc: 0.9575 - val_loss: 0.9069 - val_acc: 0.9572\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8968 - acc: 0.9580 - val_loss: 0.9053 - val_acc: 0.9557\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8949 - acc: 0.9576 - val_loss: 0.9191 - val_acc: 0.9522\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.8965 - acc: 0.9565 - val_loss: 0.8855 - val_acc: 0.9620\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8958 - acc: 0.9574 - val_loss: 0.8886 - val_acc: 0.9604\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8914 - acc: 0.9586 - val_loss: 0.8947 - val_acc: 0.9583\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 1.8216 - acc: 0.8207 - val_loss: 1.0304 - val_acc: 0.9439\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.8667 - acc: 0.9323 - val_loss: 0.6385 - val_acc: 0.9559\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.5547 - acc: 0.9499 - val_loss: 0.4286 - val_acc: 0.9658\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4118 - acc: 0.9588 - val_loss: 0.3477 - val_acc: 0.9678\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3355 - acc: 0.9650 - val_loss: 0.2852 - val_acc: 0.9730\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.2935 - acc: 0.9673 - val_loss: 0.3127 - val_acc: 0.9584\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.2650 - acc: 0.9701 - val_loss: 0.2387 - val_acc: 0.9747\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2372 - acc: 0.9736 - val_loss: 0.2273 - val_acc: 0.9746\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.2225 - acc: 0.9747 - val_loss: 0.2177 - val_acc: 0.9763\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.2061 - acc: 0.9777 - val_loss: 0.1975 - val_acc: 0.9776\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.2010 - acc: 0.9770 - val_loss: 0.2032 - val_acc: 0.9749\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1876 - acc: 0.9794 - val_loss: 0.1814 - val_acc: 0.9817\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1797 - acc: 0.9802 - val_loss: 0.2129 - val_acc: 0.9677\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1741 - acc: 0.9807 - val_loss: 0.2746 - val_acc: 0.9438\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1655 - acc: 0.9820 - val_loss: 0.1758 - val_acc: 0.9780\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1636 - acc: 0.9815 - val_loss: 0.2012 - val_acc: 0.9700\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1568 - acc: 0.9834 - val_loss: 0.2234 - val_acc: 0.9640\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1517 - acc: 0.9834 - val_loss: 0.1914 - val_acc: 0.9720\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.1519 - acc: 0.9833 - val_loss: 0.1639 - val_acc: 0.9779\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1475 - acc: 0.9829 - val_loss: 0.1628 - val_acc: 0.9776\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1424 - acc: 0.9847 - val_loss: 0.2247 - val_acc: 0.9583\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1406 - acc: 0.9850 - val_loss: 0.2071 - val_acc: 0.9625\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.1351 - acc: 0.9864 - val_loss: 0.1636 - val_acc: 0.9776\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1357 - acc: 0.9855 - val_loss: 0.1622 - val_acc: 0.9771\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.1317 - acc: 0.9856 - val_loss: 0.1543 - val_acc: 0.9778\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1290 - acc: 0.9861 - val_loss: 0.2049 - val_acc: 0.9637\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1247 - acc: 0.9879 - val_loss: 0.1911 - val_acc: 0.9672\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1234 - acc: 0.9875 - val_loss: 0.1495 - val_acc: 0.9786\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.1234 - acc: 0.9876 - val_loss: 0.1769 - val_acc: 0.9703\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1211 - acc: 0.9872 - val_loss: 0.1494 - val_acc: 0.9792\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.1204 - acc: 0.9878 - val_loss: 0.1587 - val_acc: 0.9745\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1196 - acc: 0.9876 - val_loss: 0.1523 - val_acc: 0.9774\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1147 - acc: 0.9886 - val_loss: 0.1719 - val_acc: 0.9733\n",
      "Epoch 34/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1155 - acc: 0.9886 - val_loss: 0.1594 - val_acc: 0.9766\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1128 - acc: 0.9891 - val_loss: 0.1734 - val_acc: 0.9701\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1151 - acc: 0.9880 - val_loss: 0.1645 - val_acc: 0.9738\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1104 - acc: 0.9895 - val_loss: 0.1424 - val_acc: 0.9789\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1143 - acc: 0.9879 - val_loss: 0.1442 - val_acc: 0.9791\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.1098 - acc: 0.9894 - val_loss: 0.1383 - val_acc: 0.9793\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1072 - acc: 0.9894 - val_loss: 0.1776 - val_acc: 0.9691\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1116 - acc: 0.9881 - val_loss: 0.1464 - val_acc: 0.9770\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1098 - acc: 0.9889 - val_loss: 0.1353 - val_acc: 0.9811\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1079 - acc: 0.9890 - val_loss: 0.1517 - val_acc: 0.9754\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1051 - acc: 0.9899 - val_loss: 0.1839 - val_acc: 0.9675\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.1069 - acc: 0.9888 - val_loss: 0.1414 - val_acc: 0.9775\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1079 - acc: 0.9891 - val_loss: 0.1562 - val_acc: 0.9750\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.1054 - acc: 0.9893 - val_loss: 0.1474 - val_acc: 0.9762\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1044 - acc: 0.9892 - val_loss: 0.1326 - val_acc: 0.9815\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1041 - acc: 0.9895 - val_loss: 0.1324 - val_acc: 0.9833\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.1001 - acc: 0.9908 - val_loss: 0.1435 - val_acc: 0.9786\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1023 - acc: 0.9900 - val_loss: 0.1491 - val_acc: 0.9775\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1005 - acc: 0.9899 - val_loss: 0.2064 - val_acc: 0.9561\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.1043 - acc: 0.9894 - val_loss: 0.1372 - val_acc: 0.9797\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0989 - acc: 0.9902 - val_loss: 0.1337 - val_acc: 0.9793\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0983 - acc: 0.9900 - val_loss: 0.1340 - val_acc: 0.9801\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1039 - acc: 0.9890 - val_loss: 0.1406 - val_acc: 0.9768\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1003 - acc: 0.9901 - val_loss: 0.1291 - val_acc: 0.9803\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.1008 - acc: 0.9893 - val_loss: 0.1349 - val_acc: 0.9798\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0985 - acc: 0.9901 - val_loss: 0.1467 - val_acc: 0.9762\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0991 - acc: 0.9907 - val_loss: 0.1376 - val_acc: 0.9771\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0979 - acc: 0.9901 - val_loss: 0.1427 - val_acc: 0.9760\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0979 - acc: 0.9899 - val_loss: 0.1347 - val_acc: 0.9782\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0954 - acc: 0.9905 - val_loss: 0.1385 - val_acc: 0.9780\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0929 - acc: 0.9915 - val_loss: 0.1479 - val_acc: 0.9765\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0939 - acc: 0.9911 - val_loss: 0.1508 - val_acc: 0.9750\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0973 - acc: 0.9907 - val_loss: 0.1253 - val_acc: 0.9815\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0918 - acc: 0.9914 - val_loss: 0.2069 - val_acc: 0.9609\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0925 - acc: 0.9911 - val_loss: 0.1484 - val_acc: 0.9751\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0929 - acc: 0.9911 - val_loss: 0.1317 - val_acc: 0.9788\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0945 - acc: 0.9902 - val_loss: 0.1498 - val_acc: 0.9760\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0901 - acc: 0.9917 - val_loss: 0.1719 - val_acc: 0.9692\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0962 - acc: 0.9900 - val_loss: 0.1260 - val_acc: 0.9811\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0929 - acc: 0.9908 - val_loss: 0.1303 - val_acc: 0.9801\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0939 - acc: 0.9909 - val_loss: 0.1231 - val_acc: 0.9822\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0928 - acc: 0.9907 - val_loss: 0.1313 - val_acc: 0.9799\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0899 - acc: 0.9915 - val_loss: 0.1334 - val_acc: 0.9794\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0875 - acc: 0.9919 - val_loss: 0.1344 - val_acc: 0.9797\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0948 - acc: 0.9899 - val_loss: 0.1285 - val_acc: 0.9812\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0918 - acc: 0.9908 - val_loss: 0.1274 - val_acc: 0.9807\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0885 - acc: 0.9917 - val_loss: 0.2572 - val_acc: 0.9482\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0925 - acc: 0.9904 - val_loss: 0.1484 - val_acc: 0.9759\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0900 - acc: 0.9914 - val_loss: 0.1366 - val_acc: 0.9786\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0945 - acc: 0.9904 - val_loss: 0.1435 - val_acc: 0.9754\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0914 - acc: 0.9906 - val_loss: 0.1310 - val_acc: 0.9801\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0909 - acc: 0.9910 - val_loss: 0.1246 - val_acc: 0.9807\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0877 - acc: 0.9918 - val_loss: 0.1346 - val_acc: 0.9770\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0905 - acc: 0.9907 - val_loss: 0.1489 - val_acc: 0.9758\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0877 - acc: 0.9910 - val_loss: 0.1690 - val_acc: 0.9702\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0928 - acc: 0.9901 - val_loss: 0.1588 - val_acc: 0.9713\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0906 - acc: 0.9912 - val_loss: 0.1239 - val_acc: 0.9818\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0897 - acc: 0.9908 - val_loss: 0.1337 - val_acc: 0.9779\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0853 - acc: 0.9922 - val_loss: 0.1819 - val_acc: 0.9653\n",
      "Epoch 93/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0882 - acc: 0.9916 - val_loss: 0.1345 - val_acc: 0.9792\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0864 - acc: 0.9914 - val_loss: 0.1342 - val_acc: 0.9779\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0871 - acc: 0.9912 - val_loss: 0.1417 - val_acc: 0.9763\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0878 - acc: 0.9914 - val_loss: 0.1272 - val_acc: 0.9806\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0880 - acc: 0.9911 - val_loss: 0.1282 - val_acc: 0.9795\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0856 - acc: 0.9914 - val_loss: 0.1267 - val_acc: 0.9804\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0867 - acc: 0.9916 - val_loss: 0.1321 - val_acc: 0.9780\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0850 - acc: 0.9917 - val_loss: 0.1441 - val_acc: 0.9768\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0821 - acc: 0.9928 - val_loss: 0.1541 - val_acc: 0.9725\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0875 - acc: 0.9910 - val_loss: 0.1711 - val_acc: 0.9698\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0846 - acc: 0.9912 - val_loss: 0.1368 - val_acc: 0.9766\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0872 - acc: 0.9913 - val_loss: 0.1305 - val_acc: 0.9795\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0853 - acc: 0.9921 - val_loss: 0.1274 - val_acc: 0.9797\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0881 - acc: 0.9912 - val_loss: 0.1413 - val_acc: 0.9766\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0821 - acc: 0.9923 - val_loss: 0.1531 - val_acc: 0.9748\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0847 - acc: 0.9917 - val_loss: 0.1248 - val_acc: 0.9811\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0887 - acc: 0.9907 - val_loss: 0.1200 - val_acc: 0.9809\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0827 - acc: 0.9923 - val_loss: 0.1526 - val_acc: 0.9714\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0845 - acc: 0.9912 - val_loss: 0.1593 - val_acc: 0.9704\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0817 - acc: 0.9923 - val_loss: 0.1238 - val_acc: 0.9802\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0861 - acc: 0.9904 - val_loss: 0.1434 - val_acc: 0.9755\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0854 - acc: 0.9912 - val_loss: 0.2018 - val_acc: 0.9629\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0827 - acc: 0.9919 - val_loss: 0.1458 - val_acc: 0.9745\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0846 - acc: 0.9912 - val_loss: 0.1272 - val_acc: 0.9800\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0838 - acc: 0.9915 - val_loss: 0.1766 - val_acc: 0.9699\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0799 - acc: 0.9932 - val_loss: 0.1315 - val_acc: 0.9776\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0840 - acc: 0.9907 - val_loss: 0.1255 - val_acc: 0.9821\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0827 - acc: 0.9921 - val_loss: 0.1291 - val_acc: 0.9793\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0849 - acc: 0.9918 - val_loss: 0.1261 - val_acc: 0.9800\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0831 - acc: 0.9918 - val_loss: 0.1342 - val_acc: 0.9779\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.0819 - acc: 0.9922 - val_loss: 0.1611 - val_acc: 0.9707\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0855 - acc: 0.9909 - val_loss: 0.1703 - val_acc: 0.9693\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0814 - acc: 0.9921 - val_loss: 0.1248 - val_acc: 0.9802\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0823 - acc: 0.9919 - val_loss: 0.1209 - val_acc: 0.9810\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0805 - acc: 0.9922 - val_loss: 0.1280 - val_acc: 0.9802\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0846 - acc: 0.9914 - val_loss: 0.2008 - val_acc: 0.9637\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0795 - acc: 0.9926 - val_loss: 0.1563 - val_acc: 0.9716\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0873 - acc: 0.9908 - val_loss: 0.1649 - val_acc: 0.9708\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0815 - acc: 0.9917 - val_loss: 0.1291 - val_acc: 0.9786\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0841 - acc: 0.9911 - val_loss: 0.1269 - val_acc: 0.9788\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0795 - acc: 0.9926 - val_loss: 0.1395 - val_acc: 0.9781\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0826 - acc: 0.9920 - val_loss: 0.1422 - val_acc: 0.9757\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0821 - acc: 0.9918 - val_loss: 0.1244 - val_acc: 0.9812\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0779 - acc: 0.9926 - val_loss: 0.1989 - val_acc: 0.9630\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0799 - acc: 0.9927 - val_loss: 0.1375 - val_acc: 0.9764\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0788 - acc: 0.9926 - val_loss: 0.1741 - val_acc: 0.9669\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0852 - acc: 0.9910 - val_loss: 0.1226 - val_acc: 0.9821\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0815 - acc: 0.9922 - val_loss: 0.1290 - val_acc: 0.9793\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0800 - acc: 0.9918 - val_loss: 0.1335 - val_acc: 0.9792\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0758 - acc: 0.9937 - val_loss: 0.1259 - val_acc: 0.9808\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0837 - acc: 0.9910 - val_loss: 0.1338 - val_acc: 0.9776\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0818 - acc: 0.9912 - val_loss: 0.1313 - val_acc: 0.9800\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.0786 - acc: 0.9924 - val_loss: 0.1363 - val_acc: 0.9773\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0825 - acc: 0.9911 - val_loss: 0.1261 - val_acc: 0.9806\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0844 - acc: 0.9911 - val_loss: 0.1248 - val_acc: 0.9809\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0799 - acc: 0.9923 - val_loss: 0.1255 - val_acc: 0.9800\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0815 - acc: 0.9918 - val_loss: 0.1573 - val_acc: 0.9710\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0780 - acc: 0.9928 - val_loss: 0.1227 - val_acc: 0.9810\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0796 - acc: 0.9920 - val_loss: 0.1265 - val_acc: 0.9801\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0831 - acc: 0.9915 - val_loss: 0.1228 - val_acc: 0.9808\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0765 - acc: 0.9927 - val_loss: 0.1405 - val_acc: 0.9784\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0804 - acc: 0.9922 - val_loss: 0.1411 - val_acc: 0.9748\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0808 - acc: 0.9919 - val_loss: 0.1352 - val_acc: 0.9769\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0819 - acc: 0.9917 - val_loss: 0.1625 - val_acc: 0.9725\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0794 - acc: 0.9919 - val_loss: 0.1219 - val_acc: 0.9816\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0850 - acc: 0.9908 - val_loss: 0.1230 - val_acc: 0.9803\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0799 - acc: 0.9923 - val_loss: 0.1508 - val_acc: 0.9732\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0786 - acc: 0.9922 - val_loss: 0.1576 - val_acc: 0.9725\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0818 - acc: 0.9909 - val_loss: 0.1292 - val_acc: 0.9792\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0753 - acc: 0.9933 - val_loss: 0.1489 - val_acc: 0.9727\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0800 - acc: 0.9913 - val_loss: 0.1307 - val_acc: 0.9786\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0787 - acc: 0.9920 - val_loss: 0.1279 - val_acc: 0.9788\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0785 - acc: 0.9923 - val_loss: 0.1376 - val_acc: 0.9764\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0806 - acc: 0.9913 - val_loss: 0.1167 - val_acc: 0.9820\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0787 - acc: 0.9920 - val_loss: 0.1212 - val_acc: 0.9803\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0764 - acc: 0.9928 - val_loss: 0.1301 - val_acc: 0.9790\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0766 - acc: 0.9927 - val_loss: 0.1312 - val_acc: 0.9791\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0805 - acc: 0.9918 - val_loss: 0.1369 - val_acc: 0.9760\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0783 - acc: 0.9920 - val_loss: 0.1293 - val_acc: 0.9794\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0768 - acc: 0.9926 - val_loss: 0.1561 - val_acc: 0.9688\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0767 - acc: 0.9926 - val_loss: 0.1499 - val_acc: 0.9740\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.0784 - acc: 0.9919 - val_loss: 0.1348 - val_acc: 0.9774\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0787 - acc: 0.9921 - val_loss: 0.1331 - val_acc: 0.9762\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0784 - acc: 0.9915 - val_loss: 0.1315 - val_acc: 0.9785\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0780 - acc: 0.9929 - val_loss: 0.1246 - val_acc: 0.9804\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0756 - acc: 0.9925 - val_loss: 0.1440 - val_acc: 0.9748\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0772 - acc: 0.9924 - val_loss: 0.1325 - val_acc: 0.9778\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0765 - acc: 0.9923 - val_loss: 0.1899 - val_acc: 0.9647\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0786 - acc: 0.9920 - val_loss: 0.1239 - val_acc: 0.9799\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0788 - acc: 0.9916 - val_loss: 0.1403 - val_acc: 0.9755\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0780 - acc: 0.9922 - val_loss: 0.1325 - val_acc: 0.9770\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0774 - acc: 0.9920 - val_loss: 0.1380 - val_acc: 0.9766\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0799 - acc: 0.9916 - val_loss: 0.1182 - val_acc: 0.9804\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0750 - acc: 0.9929 - val_loss: 0.1208 - val_acc: 0.9796\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0769 - acc: 0.9923 - val_loss: 0.1364 - val_acc: 0.9758\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0802 - acc: 0.9917 - val_loss: 0.1281 - val_acc: 0.9778\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0753 - acc: 0.9927 - val_loss: 0.1289 - val_acc: 0.9766\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0831 - acc: 0.9901 - val_loss: 0.1250 - val_acc: 0.9806\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0726 - acc: 0.9939 - val_loss: 0.1529 - val_acc: 0.9735\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.0774 - acc: 0.9919 - val_loss: 0.2308 - val_acc: 0.9514\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0782 - acc: 0.9920 - val_loss: 0.3514 - val_acc: 0.9330\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0766 - acc: 0.9931 - val_loss: 0.1233 - val_acc: 0.9806\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0756 - acc: 0.9929 - val_loss: 0.1342 - val_acc: 0.9777\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0765 - acc: 0.9922 - val_loss: 0.1216 - val_acc: 0.9808\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.0749 - acc: 0.9929 - val_loss: 0.1450 - val_acc: 0.9739\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 1s 21us/step - loss: 0.0776 - acc: 0.9920 - val_loss: 0.2953 - val_acc: 0.9382\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 1s 21us/step - loss: 0.0759 - acc: 0.9921 - val_loss: 0.1373 - val_acc: 0.9751\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.0772 - acc: 0.9929 - val_loss: 0.1624 - val_acc: 0.9729\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build artchicture - L1 regularization# build  \n",
    "# build artchicture\n",
    "\n",
    "l1_network = models.Sequential()\n",
    "l1_network.add(layers.Dense(512, activation='relu', kernel_regularizer=l1(0.001), input_shape=(28 * 28,)))\n",
    "\n",
    "\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "\n",
    "l1_network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# build artchicture - L2 regularization\n",
    "l2_network = models.Sequential()\n",
    "l2_network.add(layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001), input_shape=(28 * 28,)))\n",
    "\n",
    "\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "\n",
    "\n",
    "l2_network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# Fit the model# Fit th \n",
    "l1_network.compile(optimizer='rmsprop', loss='categorical_crossentropy',  metrics=['acc'])\n",
    "l1_fit = l1_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n",
    "l2_network.compile(optimizer='rmsprop', loss='categorical_crossentropy',  metrics=['acc'])\n",
    "l2_fit = l2_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f560403ec88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f563c97bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot the training losses of all 4 models# Plot t \n",
    "l1_val_error = l1_fit.history['val_loss']\n",
    "l1_val_acc = l1_fit.history['val_acc']\n",
    "\n",
    "l2_val_error = l2_fit.history['val_loss']\n",
    "l2_val_acc = l2_fit.history['val_acc']\n",
    "\n",
    "plt.plot(x_vals, drop_val_error, label='Dropped' )\n",
    "plt.plot(x_vals, vali_error, label='Original')\n",
    "plt.plot(x_vals, l1_val_error, label='L1')\n",
    "plt.plot(x_vals, l2_val_error, label='L2')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.title(\"Validation error by Epoch across Models\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07581293846368789,\n",
       " 0.080298995447158808,\n",
       " 0.88546806755065921,\n",
       " 0.11667728378772736)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min1=np.min(drop_val_error)\n",
    "min2=np.min(vali_error)\n",
    "min3=np.min(l1_val_error)\n",
    "min4=np.min(l2_val_error)\n",
    "min1,min2,min3,min4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is the dropped one, with the lowest error rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal epoch for the main baseline model is: 5\n"
     ]
    }
   ],
   "source": [
    "new_epoch_count =  np.argmin(vali_error) + 1\n",
    "print(\"The optimal epoch for the main baseline model is:\", new_epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0528 - acc: 0.9951\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0322 - acc: 0.9962\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0246 - acc: 0.9970\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0215 - acc: 0.9973\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0177 - acc: 0.9978\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "Test accuracy: 0.984\n",
      "Test loss: 0.147110126\n"
     ]
    }
   ],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "full_fit = network.fit(train_images, train_labels, epochs=new_epoch_count, batch_size=batchsize)\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std \n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer_list):\n",
    "    '''\n",
    "    The layer_list is a list of integers, each denoting a layer to\n",
    "    add with the corresponding number of nodes\n",
    "    '''\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    for node_count in layer_list:\n",
    "        model.add(layers.Dense(node_count, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cross_validate(model_structure):\n",
    "    '''\n",
    "    Given a model structure, returns the avg MSE over 10 k-fold\n",
    "    cross validation training\n",
    "    \n",
    "    This facilitates rapid testing of models\n",
    "    '''\n",
    "    \n",
    "    kf = KFold(n_splits = k)\n",
    "    cur_fold = 0\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        cur_fold +=1 \n",
    "\n",
    "        # Partition the data\n",
    "        x_train, x_test = train_data[train_index], train_data[test_index]\n",
    "        y_train, y_test = train_targets[train_index], train_targets[test_index]\n",
    "\n",
    "\n",
    "        model = build_model(model_structure)\n",
    "        model.fit(x_train, y_train, epochs=num_epochs, batch_size=1)\n",
    "        val_mse, val_mae = model.evaluate(x_test, y_test)\n",
    "        all_scores.append(val_mse)\n",
    "        print(\"K-FOLD = {}| MSE = {}\".format(cur_fold, val_mse ))\n",
    "\n",
    "    mean_score = np.mean(all_scores)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...........\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /software/Anaconda3-5.0.0.1-el7-x86_64:\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    conda: 4.3.30-py36h5d9f9f4_0             --> 4.5.4-py36_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "    blas:  1.1-openblas          conda-forge --> 1.0-mkl     \n",
      "\n",
      "\n",
      "CondaIOError: Missing write permissions in: /software/Anaconda3-5.0.0.1-el7-x86_64\n",
      "#\n",
      "# You don't appear to have the necessary permissions to install packages\n",
      "# into the install area '/software/Anaconda3-5.0.0.1-el7-x86_64'.\n",
      "# However you can clone this environment into your home directory and\n",
      "# then make changes to it.\n",
      "# This may be done using the command:\n",
      "#\n",
      "# $ conda create -n my_root --clone=\"/software/Anaconda3-5.0.0.1-el7-x86_64\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k = 10\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 20\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 440.8022 - mean_squared_error: 440.8022\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 169.1629 - mean_squared_error: 169.1629\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 64.0240 - mean_squared_error: 64.0240\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 43.2740 - mean_squared_error: 43.2740\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 32.9473 - mean_squared_error: 32.9473\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 27.6486 - mean_squared_error: 27.6486\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 23.8414 - mean_squared_error: 23.8414\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 21.7562 - mean_squared_error: 21.7562\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 20.0081 - mean_squared_error: 20.0081\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 18.9754 - mean_squared_error: 18.9754\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 17.6934 - mean_squared_error: 17.6934\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 16.8600 - mean_squared_error: 16.8600\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 16.2022 - mean_squared_error: 16.2022\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 15.2608 - mean_squared_error: 15.2608\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 14.8507 - mean_squared_error: 14.8507\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 14.3883 - mean_squared_error: 14.3883\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 13.9261 - mean_squared_error: 13.9261\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 13.1935 - mean_squared_error: 13.1935\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 12.8358 - mean_squared_error: 12.8358\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 12.3090 - mean_squared_error: 12.3090\n",
      "41/41 [==============================] - 0s 8ms/step\n",
      "K-FOLD = 1| MSE = 25.873221589297785\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 471.5113 - mean_squared_error: 471.5113\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 186.7925 - mean_squared_error: 186.7925\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 62.9127 - mean_squared_error: 62.9127\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 43.4348 - mean_squared_error: 43.4348\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 34.8929 - mean_squared_error: 34.8929\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 30.3418 - mean_squared_error: 30.3418\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 26.7523 - mean_squared_error: 26.7523\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 24.9038 - mean_squared_error: 24.9038\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 22.8556 - mean_squared_error: 22.8556\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 21.4482 - mean_squared_error: 21.4482\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 20.1935 - mean_squared_error: 20.1935\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 18.6754 - mean_squared_error: 18.6754\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 17.8068 - mean_squared_error: 17.8068\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 17.1176 - mean_squared_error: 17.1176\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 16.1841 - mean_squared_error: 16.1841\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 15.5783 - mean_squared_error: 15.5783\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 15.0970 - mean_squared_error: 15.0970\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 14.5191 - mean_squared_error: 14.5191\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 13.8463 - mean_squared_error: 13.8463\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 13.3262 - mean_squared_error: 13.3262\n",
      "41/41 [==============================] - 0s 9ms/step\n",
      "K-FOLD = 2| MSE = 3.3524574651950743\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 438.7925 - mean_squared_error: 438.7925\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 186.8309 - mean_squared_error: 186.8309\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 75.6315 - mean_squared_error: 75.6315\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 53.3842 - mean_squared_error: 53.3842\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 42.5367 - mean_squared_error: 42.5367\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 36.3390 - mean_squared_error: 36.3390\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 31.4292 - mean_squared_error: 31.4292\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 27.5979 - mean_squared_error: 27.5979\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 24.5208 - mean_squared_error: 24.5208\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 22.4714 - mean_squared_error: 22.4714\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 21.2624 - mean_squared_error: 21.2624\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 19.9452 - mean_squared_error: 19.9452\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 18.8702 - mean_squared_error: 18.8702\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 17.8324 - mean_squared_error: 17.8324\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 16.8475 - mean_squared_error: 16.8475\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 16.2093 - mean_squared_error: 16.2093\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 15.4176 - mean_squared_error: 15.4176\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 14.8614 - mean_squared_error: 14.8614\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 14.5707 - mean_squared_error: 14.5707\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 13.9010 - mean_squared_error: 13.9010\n",
      "41/41 [==============================] - 0s 9ms/step\n",
      "K-FOLD = 3| MSE = 10.926049279003609\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 473.5910 - mean_squared_error: 473.5910\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 208.3558 - mean_squared_error: 208.3558\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 67.7058 - mean_squared_error: 67.7058\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 44.3624 - mean_squared_error: 44.3624\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 35.5327 - mean_squared_error: 35.5327\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 30.3795 - mean_squared_error: 30.3795\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 27.0073 - mean_squared_error: 27.0073\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 24.5385 - mean_squared_error: 24.5385\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 22.6262 - mean_squared_error: 22.6262\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 21.2199 - mean_squared_error: 21.2199\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 3ms/step - loss: 20.0743 - mean_squared_error: 20.0743\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 19.0475 - mean_squared_error: 19.0475\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 18.1255 - mean_squared_error: 18.1255\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 17.4151 - mean_squared_error: 17.4151\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 16.7418 - mean_squared_error: 16.7418\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 16.1531 - mean_squared_error: 16.1531\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 15.4116 - mean_squared_error: 15.4116\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 14.8893 - mean_squared_error: 14.8893\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 14.5981 - mean_squared_error: 14.5981\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 13.7838 - mean_squared_error: 13.7838\n",
      "41/41 [==============================] - 0s 12ms/step\n",
      "K-FOLD = 4| MSE = 15.714405315678293\n",
      "Epoch 1/20\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 463.4529 - mean_squared_error: 463.4529\n",
      "Epoch 2/20\n",
      "364/364 [==============================] - 1s 3ms/step - loss: 198.7405 - mean_squared_error: 198.7405\n",
      "Epoch 3/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 76.0480 - mean_squared_error: 76.0480\n",
      "Epoch 4/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 53.8490 - mean_squared_error: 53.8490\n",
      "Epoch 5/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 44.4912 - mean_squared_error: 44.4912\n",
      "Epoch 6/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 38.2588 - mean_squared_error: 38.2588\n",
      "Epoch 7/20\n",
      "364/364 [==============================] - 2s 4ms/step - loss: 33.3291 - mean_squared_error: 33.3291\n",
      "Epoch 8/20\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 30.2497 - mean_squared_error: 30.2497\n",
      "Epoch 9/20\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 26.9487 - mean_squared_error: 26.9487\n",
      "Epoch 10/20\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 24.8617 - mean_squared_error: 24.8617\n",
      "Epoch 11/20\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 22.9374 - mean_squared_error: 22.9374\n",
      "Epoch 12/20\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 21.7960 - mean_squared_error: 21.7960\n",
      "Epoch 13/20\n",
      "364/364 [==============================] - 2s 5ms/step - loss: 20.5648 - mean_squared_error: 20.5648\n",
      "Epoch 14/20\n",
      "364/364 [==============================] - 2s 4ms/step - loss: 19.5672 - mean_squared_error: 19.5672\n",
      "Epoch 15/20\n",
      "364/364 [==============================] - 1s 3ms/step - loss: 18.7815 - mean_squared_error: 18.7815\n",
      "Epoch 16/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 17.8945 - mean_squared_error: 17.8945\n",
      "Epoch 17/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 17.4565 - mean_squared_error: 17.4565\n",
      "Epoch 18/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 16.4465 - mean_squared_error: 16.4465\n",
      "Epoch 19/20\n",
      "364/364 [==============================] - 1s 3ms/step - loss: 15.8154 - mean_squared_error: 15.8154\n",
      "Epoch 20/20\n",
      "364/364 [==============================] - 1s 3ms/step - loss: 15.3310 - mean_squared_error: 15.3310\n",
      "40/40 [==============================] - 0s 9ms/step\n",
      "K-FOLD = 5| MSE = 8.296554565429688\n",
      "Epoch 1/20\n",
      "364/364 [==============================] - 2s 6ms/step - loss: 492.7409 - mean_squared_error: 492.7409\n",
      "Epoch 2/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 226.2531 - mean_squared_error: 226.2531\n",
      "Epoch 3/20\n",
      "364/364 [==============================] - 1s 3ms/step - loss: 77.6207 - mean_squared_error: 77.6207\n",
      "Epoch 4/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 49.0971 - mean_squared_error: 49.0971\n",
      "Epoch 5/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 38.0843 - mean_squared_error: 38.0843\n",
      "Epoch 6/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 31.5289 - mean_squared_error: 31.5289\n",
      "Epoch 7/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 28.1550 - mean_squared_error: 28.1550\n",
      "Epoch 8/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 25.4009 - mean_squared_error: 25.4009\n",
      "Epoch 9/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 23.4931 - mean_squared_error: 23.4931\n",
      "Epoch 10/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 21.7513 - mean_squared_error: 21.7513\n",
      "Epoch 11/20\n",
      "364/364 [==============================] - 1s 4ms/step - loss: 20.3658 - mean_squared_error: 20.3658\n",
      "Epoch 12/20\n",
      "186/364 [==============>...............] - ETA: 0s - loss: 11.3683 - mean_squared_error: 11.3683"
     ]
    }
   ],
   "source": [
    "# Use functionality to determine optimal node count, given one layer\n",
    "node_sizes = np.arange(40, 120, 10)\n",
    "scores = []\n",
    "for count in node_sizes:\n",
    "    structure = [count]\n",
    "    score = cross_validate(structure)\n",
    "    print(\"Layer 1 has {} nodes - yields MSE = {}\".format(count, score))\n",
    "    scores.append(score)\n",
    "\n",
    "min_mse_size = node_sizes[ np.argmin(scores) ]\n",
    "print(\"With 1 layer optimal node count is:\", min_mse_size)\n",
    "\n",
    "# quick plot of MSE as function of node size\n",
    "plt.plot(node_sizes, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models=[[10,20],[100,200],[10,20,30],[100,200,300],[10,20,30,40],[100,200,300,400]\n",
    "mse_scores=[]\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    score = cross_validate(model)\n",
    "    mse_scores.append(score)\n",
    "    print (f\"Model {i} with parameters {models[i]} has MSE of {score}.\")\n",
    "\n",
    "index_mse = np.argmin(mse_scores)\n",
    "print (models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "## Part 1\n",
    "### i).\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "N = 100\n",
    "batchsize = 512\n",
    "epoch_count = 200\n",
    "split1 = 1/6\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "train_images = train_images.reshape( (60000, 28 * 28) )\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape( (10000, 28 * 28) )\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "fit = network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n",
    "\n",
    "\n",
    "training_error = fit.history['loss']\n",
    "training_acc = fit.history['acc']\n",
    "vali_error = fit.history['val_loss']\n",
    "vali_acc = fit.history['val_acc']\n",
    "x_vals = np.arange(1, epoch_count+1)\n",
    "\n",
    "plt.plot(x_vals, training_acc, 'r',label='Training Accuracy')\n",
    "plt.plot(x_vals, vali_acc, 'b',label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy by Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(x_vals, training_error, 'r',label='Training Accuracy')\n",
    "plt.plot(x_vals, vali_error, 'r',label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error by Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "index_min = np.argmin(vali_error)\n",
    "print(f\"The optimal epoch is {index_min}\".)\n",
    "\n",
    "### ii.\n",
    "\n",
    "drop_network = models.Sequential()\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "drop_network.add(layers.Dense(512, activation='relu'))\n",
    "drop_network.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "drop_network.add(layers.Dense(10, activation='softmax'))\n",
    "drop_network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "drop_fit = drop_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n",
    "drop_val_error = drop_fit.history['val_loss']\n",
    "drop_val_acc = drop_fit.history['val_acc']\n",
    "x_vals = np.arange(1, epoch_count+1)\n",
    "\n",
    "plt.plot(x_vals, drop_val_error,'r',label='Dropped')\n",
    "plt.plot(x_vals, vali_error,'b',label='Original')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.title(\"Validation Error by epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(x_vals, drop_val_acc,'r',label='Dropped')\n",
    "plt.plot(x_vals, vali_acc,'b',label='Original')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy by epoch\")\n",
    "plt.legend()\n",
    "\n",
    "The new model is doing better. \n",
    "\n",
    "### iii.\n",
    "\n",
    "\n",
    "# build artchicture - L1 regularization# build  \n",
    "# build artchicture\n",
    "\n",
    "l1_network = models.Sequential()\n",
    "l1_network.add(layers.Dense(512, activation='relu', kernel_regularizer=l1(0.001), input_shape=(28 * 28,)))\n",
    "\n",
    "\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "l1_network.add(layers.Dense(512, kernel_regularizer=l1(0.001), activation='relu'))\n",
    "\n",
    "l1_network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# build artchicture - L2 regularization\n",
    "l2_network = models.Sequential()\n",
    "l2_network.add(layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001), input_shape=(28 * 28,)))\n",
    "\n",
    "\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "l2_network.add(layers.Dense(512, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "\n",
    "\n",
    "l2_network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# Fit the model# Fit th \n",
    "l1_network.compile(optimizer='rmsprop', loss='categorical_crossentropy',  metrics=['acc'])\n",
    "l1_fit = l1_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n",
    "l2_network.compile(optimizer='rmsprop', loss='categorical_crossentropy',  metrics=['acc'])\n",
    "l2_fit = l2_network.fit(train_images, train_labels, epochs=epoch_count, batch_size=batchsize, validation_split=split1)\n",
    "\n",
    "\n",
    "# Plot the training losses of all 4 models# Plot t \n",
    "l1_val_error = l1_fit.history['val_loss']\n",
    "l1_val_acc = l1_fit.history['val_acc']\n",
    "\n",
    "l2_val_error = l2_fit.history['val_loss']\n",
    "l2_val_acc = l2_fit.history['val_acc']\n",
    "\n",
    "plt.plot(x_vals, drop_val_error, label='Dropped' )\n",
    "plt.plot(x_vals, vali_error, label='Original')\n",
    "plt.plot(x_vals, l1_val_error, label='L1')\n",
    "plt.plot(x_vals, l2_val_error, label='L2')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.title(\"Validation error by Epoch across Models\")\n",
    "plt.legend()\n",
    "\n",
    "min1=np.min(drop_val_error)\n",
    "min2=np.min(vali_error)\n",
    "min3=np.min(l1_val_error)\n",
    "min4=np.min(l2_val_error)\n",
    "min1,min2,min3,min4\n",
    "\n",
    "The best model is \n",
    "\n",
    "new_epoch_count =  np.argmin(vali_error) + 1\n",
    "print(\"The optimal epoch for the main baseline model is:\", new_epoch_count)\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "full_fit = network.fit(train_images, train_labels, epochs=new_epoch_count, batch_size=batchsize)\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std \n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std\n",
    "\n",
    "def build_model(layer_list):\n",
    "    '''\n",
    "    The layer_list is a list of integers, each denoting a layer to\n",
    "    add with the corresponding number of nodes\n",
    "    '''\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    for node_count in layer_list:\n",
    "        model.add(layers.Dense(node_count, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cross_validate(model_structure):\n",
    "    '''\n",
    "    Given a model structure, returns the avg MSE over 10 k-fold\n",
    "    cross validation training\n",
    "    \n",
    "    This facilitates rapid testing of models\n",
    "    '''\n",
    "    \n",
    "    kf = KFold(n_splits = k)\n",
    "    cur_fold = 0\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        cur_fold +=1 \n",
    "\n",
    "        # Partition the data\n",
    "        x_train, x_test = train_data[train_index], train_data[test_index]\n",
    "        y_train, y_test = train_targets[train_index], train_targets[test_index]\n",
    "\n",
    "\n",
    "        model = build_model(model_structure)\n",
    "        model.fit(x_train, y_train, epochs=num_epochs, batch_size=1)\n",
    "        val_mse, val_mae = model.evaluate(x_test, y_test)\n",
    "        all_scores.append(val_mse)\n",
    "        print(\"K-FOLD = {}| MSE = {}\".format(cur_fold, val_mse ))\n",
    "\n",
    "    mean_score = np.mean(all_scores)\n",
    "    return mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "k = 10\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 5\n",
    "all_scores = []\n",
    "\n",
    "# Use functionality to determine optimal node count, given one layer\n",
    "node_sizes = np.arange(40, 120, 10)\n",
    "scores = []\n",
    "for count in node_sizes:\n",
    "    structure = [count]\n",
    "    score = cross_validate(structure)\n",
    "    print(\"Layer 1 has {} nodes - yields MSE = {}\".format(count, score))\n",
    "    scores.append(score)\n",
    "\n",
    "min_mse_size = node_sizes[ np.argmin(scores) ]\n",
    "print(\"With 1 layer optimal node count is:\", min_mse_size)\n",
    "\n",
    "\n",
    "# quick plot of MSE as function of node size\n",
    "plt.plot(node_sizes, scores)\n",
    "plt.xlabel(\"Node Size\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Node Sizes VS Scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "structures=[[10,20],[100,200],[10,20,30],[100,200,300],[10,20,30,40],[100,200,300,400]]\n",
    "mse_scores=[]\n",
    "\n",
    "for i,structure in enumerate(structures):\n",
    "    score = cross_validate(structure)\n",
    "    mse_scores.append(score)\n",
    "    print (f\"Model {i} with parameters {strcutures[i]} has MSE of {score}.\")\n",
    "\n",
    "index_mse = np.argmin(mse_scores)\n",
    "print (structures[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
